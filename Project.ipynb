{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "GBQHjXdPu9A0",
        "outputId": "4bbde9a1-9e6c-4bd9-a71d-3828c57c5a97"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fea359404e75>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import config\n",
        "from model import Model\n",
        "\n",
        "episode_reward = 0 # for info and debugging\n",
        "buffer = deque([],config.BUFFER_SIZE) # Past experience arranged as a queue\n",
        "epsilon = config.MAX_EPSILON\n",
        "alpha = config.ALPHA\n",
        "decay = config.EPSILON_DECAY\n",
        "#to plot graphics\n",
        "cum_reward_table = np.zeros(config.NUM_EPISODES)\n",
        "cum_reward_nn = np.zeros(config.NUM_EPISODES)\n",
        "\n",
        "model = Model().to(config.DEVICE)\n",
        "#print(model)\n",
        "\n",
        "target_model = Model().to(config.DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.LR)\n",
        "optimizer_target = optim.Adam(target_model.parameters(), lr=config.LR)\n",
        "\n",
        "huber_loss=nn.HuberLoss(delta=1.0)\n",
        "\n",
        "# Define the policy to know how chose the action\n",
        "#EPSGREEDY POLICY  q table\n",
        "def select_action(state, epsilon):\n",
        "    rv = random.uniform(0, 1)\n",
        "    if rv < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "#EPSGREEDY POLICY  neural network\n",
        "def select_action_nn(state, epsilon):\n",
        "    rv = random.uniform(0, 1)\n",
        "    if rv < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        prediction = model(torch.from_numpy(state)).detach().numpy\n",
        "        action=np.argmax(prediction) # Select action with max predicted Q-value\n",
        "        return action\n",
        "\n",
        "\n",
        "## update the epsilon value along the iteration until converges to MIN_EPSILON\n",
        "def update_epsilon(epsilon):\n",
        "    epsilon -= epsilon/100 # reduce epsilon by 1/100\n",
        "    if epsilon<=config.MIN_EPSILON:\n",
        "        return config.MIN_EPSILON\n",
        "    else:\n",
        "        return epsilon\n",
        "\n",
        "## update the epsilon every episode by epsilon decay variable\n",
        "def update_epsilon_nn(epsilon):\n",
        "    epsilon *= decay\n",
        "    if epsilon<=config.MIN_EPSILON:\n",
        "        return config.MIN_EPSILON\n",
        "    else:\n",
        "        return epsilon\n",
        "\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "\n",
        "if(config.use_qtable):\n",
        "    # define the Q table\n",
        "    #Q = np.zeros([27684, env.action_space.n]) # little discretization\n",
        "    Q = np.zeros([19051200, env.action_space.n]) #big discretization\n",
        "\n",
        "###see the limit of the values of the box observation space\n",
        "#print(env.observation_space.high)\n",
        "#print(env.observation_space.low)\n",
        "\n",
        "###see in more detail the action space and the observation space\n",
        "#print(env.action_space)\n",
        "#print(env.observation_space)\n",
        "\n",
        "\n",
        "if(config.use_qtable): # use a q table to reach the goal\n",
        "    for i in range(config.NUM_EPISODES):\n",
        "        observation, info = env.reset()# use seed to have same initial state\n",
        "        #state = config.discretize(observation)\n",
        "        state = config.big_discretize(observation)\n",
        "        for j in range(500):\n",
        "            action = select_action(state,epsilon)\n",
        "            obv, reward, done, truncated, info = env.step(action)\n",
        "            #next_state = config.discretize(obv)\n",
        "            next_state = config.big_discretize(obv)\n",
        "\n",
        "            next_max = np.max(Q[next_state])\n",
        "\n",
        "            Q[state,action] += alpha*(reward+config.GAMMA*next_max-Q[state,action])\n",
        "            state = next_state\n",
        "\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        print(\"episode: \", i)\n",
        "        print(\"episode cumulative reward : \", episode_reward)\n",
        "        print(\"epsilon: \",epsilon)\n",
        "        epsilon = update_epsilon(epsilon)\n",
        "        cum_reward_table[i]=episode_reward\n",
        "        episode_reward = 0 #reset the total reward each episode\n",
        "\n",
        "    #save the q table for testing\n",
        "    #np.savetxt('q_table.csv', Q, delimiter=','fmt='%f18')\n",
        "    #np.savetxt('q_table_little_discretization2000.csv', Q, delimiter=',') # full precision\n",
        "    np.savetxt('q_table_big_discretization1000.csv', Q, delimiter=',') # full precision\n",
        "\n",
        "else: #use a nn to approximate the q function\n",
        "    for i in range(config.NUM_EPISODES):\n",
        "        state, info = env.reset()\n",
        "        for j in range(500):\n",
        "            action = select_action_nn(state,epsilon)\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            episode_reward += reward\n",
        "\n",
        "            #Remove the oldest item if the queue is full so can add new one\n",
        "            if len(buffer)>=config.BUFFER_SIZE:\n",
        "                buffer.popleft() # dequeue oldest item\n",
        "\n",
        "\n",
        "            buffer.append([*state,action,reward,*next_state,done])\n",
        "\n",
        "            state = next_state # update current state\n",
        "\n",
        "            if done or truncated:\n",
        "\n",
        "                # train NN every 4 episodes and if buffer has at least BATCH_SIZE tuple\n",
        "                if len(buffer) >= config.BATCH_SIZE and ((i+1) % 4 == 0):\n",
        "                    batch = random.sample(buffer, config.BATCH_SIZE)\n",
        "                    dataset = np.array(batch)\n",
        "                    states = torch.from_numpy((dataset[:,:8]).astype('float32'))\n",
        "                    actions = torch.from_numpy(dataset[:,8:9].astype('int64'))\n",
        "                    rewards = torch.from_numpy(dataset[:,9:10].astype('float32'))\n",
        "                    next_states = torch.from_numpy((dataset[:,10:18]).astype('float32'))\n",
        "                    dones = torch.from_numpy(dataset[:,18:19].astype('float32'))\n",
        "\n",
        "                    #-------vanilla dqn------------#\n",
        "\n",
        "                    \"\"\"# Find next best action so can compute the next reward for the target\n",
        "                    #predictions_next = target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "                    #next_actions=np.argmax(predictions_next) # Select action with max Q-value\n",
        "\n",
        "                    #Compute corresponding (predicted) reward of next state\n",
        "                    #next_rewards = predictions_next[next_actions]\n",
        "                    next_rewards = target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "                    #-------------------------------#\"\"\"\n",
        "                    #---------double dqn-------------#\n",
        "\n",
        "                    # Find next best action using model network\n",
        "                    predictions_next = model(next_states).detach().numpy()\n",
        "                    next_actions = np.argmax(predictions_next,axis=1) # Select action with max Q-value\n",
        "                    next_actions =  next_actions[..., np.newaxis]\n",
        "\n",
        "                    #evaluate Q(s',a') founded by model using the target network\n",
        "                    next_rewards = target_model(next_states).gather(1, torch.from_numpy(next_actions))\n",
        "                    #next_rewards = torch.from_numpy(evaluations[next_actions])\n",
        "\n",
        "                    #-------------------------------#\n",
        "\n",
        "                    targets = rewards + config.GAMMA_NN*next_rewards*(1-dones)\n",
        "\n",
        "                    #compute the predicted value of the model(output)\n",
        "                    output=model(states).gather(1, actions)\n",
        "                    #compute the huber loss\n",
        "                    loss = huber_loss(output, targets)\n",
        "                    #Train network\n",
        "                    optimizer.zero_grad()#clear existing gradient\n",
        "                    loss.backward() #backpropagate the error\n",
        "                    optimizer.step() # update weights\n",
        "                    #save the weight of the network\n",
        "                    config.save_model(model,optimizer,i+1)\n",
        "                    print(\"Save weigths in: \"+ config.CHECKPOINT)\n",
        "                    epsilon = update_epsilon_nn(epsilon)\n",
        "\n",
        "                #update weights of target network every 10 episodes\n",
        "                if  (i+1) % config.TARGET_FREQ_UPDATE == 0:\n",
        "                    print(\"Target network updated\")\n",
        "                    config.load_model(config.CHECKPOINT,target_model,optimizer_target)\n",
        "\n",
        "                print(\"episode \", i)\n",
        "                print(\"episode cumulative reward: \", episode_reward)\n",
        "                print(\"current epsilon: \", epsilon)\n",
        "                print(\"#---------------------------------------------#\")\n",
        "                break\n",
        "\n",
        "        cum_reward_nn[i]=episode_reward\n",
        "        episode_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "###### after saved results of the test plot it\n",
        "#first create an array with natural number to represent the episodes\n",
        "ep = np.zeros(config.NUM_EPISODES,int)\n",
        "for i in range (config.NUM_EPISODES):\n",
        "    ep[i] = i\n",
        "\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "#fig, ax = plt.subplots()  # Create a figure containing a single axes.\n",
        "ax = fig.add_subplot(111)\n",
        "#ax.plot(ep, cum_reward_table, label=\"Q table policy\")  # Plot some data on the axes.\n",
        "ax.plot(ep, cum_reward_table, label=\"Q table policy\")  # Plot some data on the axes.\n",
        "ax.set_xlabel('EPISODES', fontsize=14)  # Add an x-label to the axes.\n",
        "ax.set_ylabel('CUMULATIVE REWARD', fontsize=14)  # Add a y-label to the axes.\n",
        "ax.set_title(\"TRAINING AGENT\", fontsize=18)  # Add a title to the axes.\n",
        "ax.legend(loc=(0.3, -0.1))  # Add a legend.\n",
        "\n",
        "#specify axis tick step sizes\n",
        "plt.xticks(np.arange(0,1000,50))# np.arange(min,max, step)\n",
        "\n",
        "#-----save image of the plot--------#\n",
        "plt.savefig('plots/Q_table_big1000.png')"
      ]
    }
  ]
}
